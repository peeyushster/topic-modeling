{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import nltk\n",
    "import json\n",
    "import gzip\n",
    "import torch\n",
    "import string\n",
    "import random\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, RandomSampler, SequentialSampler, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "gpus = [0]\n",
    "torch.cuda.set_device(gpus[0])\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if USE_CUDA else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if USE_CUDA else torch.ByteTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convCategories(categories, category_to_index):\n",
    "    return [category_to_index[category] for category in categories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate_test_split(df, train_percent=.8, validate_percent=.1, seed=None):\n",
    "    np.random.seed(seed)\n",
    "    perm = np.random.permutation(df.index)\n",
    "    m = len(df.index)\n",
    "    train_end = int(train_percent * m)\n",
    "    validate_end = int(validate_percent * m) + train_end\n",
    "    train = df.iloc[perm[:train_end]]\n",
    "    validate = df.iloc[perm[train_end:validate_end]]\n",
    "    test = df.iloc[perm[validate_end:]]\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(dataset, word_to_index):\n",
    "    _current_dictified = []\n",
    "    for l in tqdm(dataset['tokens']):\n",
    "        encoded_l = [word_to_index[i] if i in word_to_index else word_to_index['<UNK>'] for i in l]\n",
    "        _current_dictified.append(encoded_l)\n",
    "    return _current_dictified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILE = 'wikitext_tokenized.p'\n",
    "wiki_df =  pkl.load(open(OUTPUT_FILE, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = []\n",
    "for i in list(wiki_df['mid_level_categories']):\n",
    "    categories.extend(i)\n",
    "categories = list(set(categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_to_index = {categories[i]:i for i in range(0, len(categories))}\n",
    "index_to_category = {v:k for k, v in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_df['category_tokens'] = wiki_df.apply(lambda row: convCategories(row['mid_level_categories'], category_to_index), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_train, wiki_valid, wiki_test = train_validate_test_split(wiki_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = list(wiki_train['category_tokens'])\n",
    "y_val = list(wiki_valid['category_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set([y for x in list(wiki_train['tokens']) for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "605558"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {\"<PAD>\":0, \"<UNK>\":1}\n",
    "for word in vocab:\n",
    "    if(word not in word_to_index):\n",
    "        word_to_index[word]=len(word_to_index)\n",
    "index_to_word = {v:k for k, v in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82547/82547 [00:08<00:00, 9417.68it/s] \n"
     ]
    }
   ],
   "source": [
    "wiki_tokenized_train = tokenize_dataset(wiki_train, word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10318/10318 [00:02<00:00, 4042.74it/s]\n"
     ]
    }
   ],
   "source": [
    "wiki_tokenized_val = tokenize_dataset(wiki_valid, word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_tokenized_datasets = {}\n",
    "wiki_tokenized_datasets['train'] = wiki_tokenized_train\n",
    "wiki_tokenized_datasets['val'] = wiki_tokenized_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_tensor_dataset = {}\n",
    "wiki_tensor_dataset['train'] = TensoredDataset(wiki_tokenized_datasets['train'], y_train)\n",
    "wiki_tensor_dataset['val'] = TensoredDataset(wiki_tokenized_datasets['val'], y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_list_of_tensors(list_of_tensors, pad_token):\n",
    "    max_length = max([t.size(-1) for t in list_of_tensors])\n",
    "    padded_list = []\n",
    "    \n",
    "    for t in list_of_tensors:\n",
    "        #print(t.reshape(1, -1).shape)\n",
    "        #print(torch.tensor([[pad_token]*(max_length - t.size(-1))])[0].shape)\n",
    "        padded_tensor = torch.cat([t.reshape(1, -1), torch.tensor([[pad_token]*(max_length - t.size(-1))], dtype=torch.long)], dim = -1)\n",
    "        padded_list.append(padded_tensor)\n",
    "    padded_tensor = torch.cat(padded_list, dim=0)\n",
    "    return padded_tensor\n",
    "\n",
    "def pad_collate_fn(batch):\n",
    "    # batch is a list of sample tuples\n",
    "    input_list = [s[0] for s in batch]\n",
    "    target_list = [s[1] for s in batch]\n",
    "    \n",
    "    #pad_token = persona_dict.get_id('<pad>')\n",
    "    pad_token = word_to_index['<PAD>']\n",
    "    \n",
    "    input_tensor = pad_list_of_tensors(input_list, pad_token)\n",
    "    #target_tensor = pad_list_of_tensors(target_list, pad_token)\n",
    "    \n",
    "    return input_tensor, target_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_loaders = {}\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "for split, wiki_dataset in wiki_tensor_dataset.items():\n",
    "    wiki_loaders[split] = DataLoader(wiki_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (inp, target) in enumerate(wiki_loaders['train']):\n",
    "    #print(inp, target)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
